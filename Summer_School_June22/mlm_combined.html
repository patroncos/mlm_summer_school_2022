<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Introduction to Multilevel Modelling</title>
    <meta charset="utf-8" />
    <meta name="author" content="Patricio Troncoso &amp; Ana Morales-Gómez Heriot-Watt University University of Edinburgh Scottish Centre for Administrative Data Research (SCADR)" />
    <script src="mlm_combined_files/header-attrs-2.14/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# <span style="font-size: 85%; color: white">Introduction to Multilevel Modelling</span>
]
.subtitle[
## <span style="font-size: 80%;">SGSSS Summer School 2022</span>
]
.author[
### Patricio Troncoso &amp; Ana Morales-Gómez<br><span style="font-size: 60%;">Heriot-Watt University<br>University of Edinburgh<br>Scottish Centre for Administrative Data Research (SCADR)</span>
]

---





class: left, middle, inverse

# Housekeeping
  
---
# Who are we?

.pull-left[

.center[**Patricio Troncoso**
]
&lt;img src="https://i-sphere.site.hw.ac.uk/wp-content/uploads/sites/15/2021/01/PATRICIO-e1611159639790-225x300.jpg" width="25%" style="display: block; margin: auto;" /&gt;

- Research Fellow, I-SPHERE, Heriot-Watt University

- Works (mostly) in Educational research

- Twitter: [@ptronc](https://twitter.com/ptronc)

- Email: [p.troncoso@hw.ac.uk](mailto:p.troncoso@hw.ac.uk)

]

.pull-right[

.center[**Ana Morales-Gómez**]

&lt;img src="https://pbs.twimg.com/profile_images/1326811878752997376/7dvI51qy_400x400.jpg" width="30%" style="display: block; margin: auto;" /&gt;


- Research Fellow, School of Law, The University of Edinburgh 

- Works (mostly) in Criminological research

- Twitter: [@A_moralesgomez](https://twitter.com/A_moralesgomez)

- Email: [Ana.Morales@ed.ac.uk](mailto:Ana.Morales@ed.ac.uk)
]

&lt;br&gt;
We both work in the Scottish Centre for Administrative Data Research (SCADR). Find out more [here](https://www.scadr.ac.uk/)
---

## Today's programme

--
  
- 10:00-10:30 - A brief overview of linear regression
- 10:30-11:00 - Multilevel data structures and examples
- 11:00-11:15 - Break
- 11:15-11:45 - Variance components and group-specific estimates
- 11:45-12:30 - Practical 1
- 12:30-13:30 - Lunch 
- 13:30-13:50 - Accounting for individual and group characteristics: fixed effects
- 14:00-14:30 - Practical 2
- 14:30-15:00 - Multilevel modelling for binary responses
- 15:00-15:45 - Practical 3
- 15:45-16:00 - Break 
- 16:00-16:20 - Differential processes between groups: random effects
- 16:20-16:50 - Practical 4
- 16:50-17:00 - Wrap up and Finish

---
class: middle

## Today's learning goals

--

- Understand the general concept of multilevel modelling and its applications in social science research questions

--

- Understand a range of multilevel models and when to use them

--

- Specify multilevel models for continuous and binary responses using R

--

- Interpret the R output of standard multilevel models

---


class: left, middle, inverse

### &lt;font color="white"&gt;Part One: &lt;/font&gt;

# In the beginning, there was... linear regression
  
---
## Correlation

The easiest way to see the relationship between two continuous variables is to plot the values of one of them against the other.

&lt;img src="mlm_combined_files/figure-html/unnamed-chunk-3-1.png" width="40%" style="display: block; margin: auto;" /&gt;

--

This is presupposing that you have a **theoretical reason** to think they are indeed related

---
## Why is there a correlation?

--

- There might be a causal link:

  - X causes Y or vice versa
  
--

- Both variables are affected by another variable

  - We call this “confounding”

--

- Both variables measure the same thing but in different ways

--

- Last but not least…

---

### A correlation can occur purely by chance!

&lt;img src="https://media3.giphy.com/media/37QUVxlBIduYqVVAjV/giphy.gif?cid=ecf05e47c1qerhqs6rgcvc7bl45kmwz3veb4ob4wqdmr4niv&amp;rid=giphy.gif&amp;ct=g" width="65%" style="display: block; margin: auto;" /&gt;

---
## Correlation doesn’t mean…

--

- Causation

  - Trends over time can look as if there is causation, but they can be a natural process.
      
      - Reading comprehension and age

--

- Also, a low correlation (close to zero) does not mean there is no association either

  - A “non-linear” relationship might exist
  
  - There can be groups/clusters with varying associations (we'll get back to this)

---

## What is Linear Regression? (1)

--

It’s the process by which we systematise the relationship between two (or more) variables.

--

We look for the “best fit” to the data we have.

--

We obtain an equation that describes how much our dependent variable (**y**) changes as our independent variable (**x**) changes.

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/exreg1.png" width="65%" style="display: block; margin: auto;" /&gt;

---

## What is Linear Regression? (2)

--

In the presence of an association between two variables, we can fit a straight line to define the relationship

--

&lt;img src="mlm_combined_files/figure-html/unnamed-chunk-6-1.png" width="45%" style="display: block; margin: auto;" /&gt;

--

This is what we typically call "the line of best fit", because it reduces the error to the minimum


---

## What is Linear Regression? (3)


In the presence of an association between two variables, we can fit a straight line to define the relationship

&lt;img src="mlm_combined_files/figure-html/unnamed-chunk-7-1.png" width="45%" style="display: block; margin: auto;" /&gt;

--

The regression equation would be: 

$$
y = 2.07 + 3.13x + e
$$

---
## What is Linear Regression? (4)

A Simple Linear Regression model has the following form:

$$ \LARGE
y = \beta_0 + \beta_1x + e
$$

Where:

`\(y\)` is the value of the dependent variable

`\(\beta_0\)` is the intercept (point at which the line crosses the y axis)

`\(\beta_1\)` is the slope (expected increase in `\(y\)` given a one unit increase in `\(x\)`)

`\(x\)` is the value of the independent variable 

`\(e\)` is the error term


---
## How do you do this in R?

Assuming we have a dataset called "data", containing variables "y" and "x":


```r
summary(lm(y ~ x, data = data))
```


```
## 
## Call:
## lm(formula = y ~ x, data = data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.24324 -0.55252  0.01285  0.65260  2.07716 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.07135    0.09477   21.86   &lt;2e-16 ***
## x            3.13895    0.08588   36.55   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.9451 on 98 degrees of freedom
## Multiple R-squared:  0.9317,	Adjusted R-squared:  0.931 
## F-statistic:  1336 on 1 and 98 DF,  p-value: &lt; 2.2e-16
```
---
## How do you interpret R output?

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/reg1.PNG" width="100%" style="display: block; margin: auto;" /&gt;
--

- The overall mean or intercept is *2.07*

--

- A one unit increase in *x* is associated with a *3.14* increase in *y*

--

- Both coefficients are statistically significant (*p&lt;0.05*)

--

- The regression model explains *93.1%* of the variance in *y*

---
## Multiple Linear Regression

As hinted in its name, multiple linear regression is when we have more than one independent variable in our model

- We rarely ever use models with only one "x"

- How is this done? Simply add more x's to the equation:

$$ \LARGE
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 +...+\beta_n x_n + e
$$
- Interpretation remains largely similar, but:

  - Effect of one IV on DV refers to when the other IVs remain constant
  - This is why you see in papers phrases like: 
      
      - "the effect of x1 on y while adjusting/controlling for x2 and x3..."

  - Intercept is now the value of DV when all IVs are *zero*

---
## MLR example output

This is a subset of a large dataset of examination results in London schools

- The DV is "**normexam**": normalised exam score at age 16
- The IVs are 
    - "standlrt": score at age 11 on the London Reading Test (LRT) 
    - "sex": coded as girl or boy
- We used this code to run the model:

```{}
lm(normexam ~ standlrt + factor(sex), data= tutorial)
```


```
##                   Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)     -0.1031842 0.01990450 -5.183966 2.277999e-07
## standlrt         0.5905958 0.01268159 46.571132 0.000000e+00
## factor(sex)girl  0.1699601 0.02570919  6.610869 4.317291e-11
```

#### Question time!

- What is the intercept?
- What is the effect of the score at age 11 on the score at age 16?
- What is the effect of sex on the exam score at age 16?

---
## Model selection

How do you decide what variables to put in a model?

--

- First of all: Theory!

--

- Model nesting: start with what is known and add your own hypotheses after that

--

If you have multiple models with different variables, how do you decide which one is better?

--

- Theory!

--

- Statistical criteria: R Squared comparison, comparing goodness of fit criteria, the likelihood-ratio test, AIC and BIC
values, etc.

    - We'll get back to this as this is vital in MLM

---
## MLR Assumptions (1)

### Normality of residuals

--

The residual term is a continuous random variable, and as such, it should have a normal distribution.

--

This roughly means that whatever is left unexplained by our model can be thought of as “random white noise”

--

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/norm1.PNG" width="75%" style="display: block; margin: auto;" /&gt;

---
## MLR Assumptions (2)

### Homoscedasticity

--

Easier said than done (!)

--

The variation of the residuals should remain constant across the predicted values of our model

--

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://statistics.laerd.com/spss-tutorials/img/lr/heteroscedastic-relationships.png" alt="Source: Laerd Statistics" width="75%" /&gt;
&lt;p class="caption"&gt;Source: Laerd Statistics&lt;/p&gt;
&lt;/div&gt;
---
## MLR Assumptions (3)

### Independence of observations

--

Observations (or individuals) should not be related to one another or somehow "clustered"

--

This is a difficult assumption to meet. Some examples:

--

- Surveys usually sample whole groups

- Individuals live in households, neighbourhoods, attend schools, etc.

    - Those are all clusters that can make them be related to each other

--

- This is where MLM comes into play!

---
class: left, middle, inverse

### &lt;font color="white"&gt;Part Two: &lt;/font&gt;

# Multilevel data structures

---
class: middle

## Why MLM?

--

Data in the real world tend to violate the assumptions of:

--

- Independence

- Homogeneity of residual variance

--

Often, in reality, data is **hierarchically structured**
---
## Data structures (1)

Individuals usually belong to groups:

&lt;img src="https://raw.githubusercontent.com/A-mora/SOCHICRIM/main/figures/cluster_prison.png" width="30%" style="display: block; margin: auto;" /&gt;
--

- Schools 

--

- Neighbourhoods

--

- Hospitals

--

- Prisons

---
## Data structures (2)

The "classic" pupils in schools structure:

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/structure1.PNG" width="100%" style="display: block; margin: auto;" /&gt;

- This is what we call a "2-level model"

- This is the typical structure of the "school value-added models"


---
## Data structures (3)

But the structure doesn't need to be composed of individuals in groups:


&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/structure2.PNG" width="100%" style="display: block; margin: auto;" /&gt;

It can also be repeated measures nested within individuals

- This is a longitudinal model structure
- It can be referred to as a "growth curve model" or as "the multilevel model for change"

---
## Data structures (4)

.pull-left[

But we are not limited to 2 levels only:


&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/structure3.PNG" width="100%" style="display: block; margin: auto;" /&gt;
- This is a 3-level structure of pupils nested within classrooms, nested within schools.

]

--

.pull-right[

We can go even further, as it was done here:

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/structure4.PNG" width="100%" style="display: block; margin: auto;" /&gt;

- In this 4-level structure, schools are nested within a higher level of local authorities.

]
---
## Data structures (5)

--

- We are not limited to one outcome either


&lt;img src="https://ars.els-cdn.com/content/image/1-s2.0-S0022440521000546-gr2.jpg" width="80%" style="display: block; margin: auto;" /&gt;

- The structure here is longitudinal and multivariate: measures of children's mental health over time

Image Source: [Troncoso and Humphrey, 2021](https://doi.org/10.1016/j.jsp.2021.08.002)

---
## Data structures (6)

--

- We are not limited to level-1 units (individuals) belonging to one level-2 unit (groups) only:


&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/structure6.PNG" width="40%" style="display: block; margin: auto;" /&gt;

- The structure here is people nested in multiple "cliques": Multiple Membership and Multiple Classification (MMMC) Models and Multilevel Social Networks

Image Source: [Tranmer, 2010](https://www.bristol.ac.uk/media-library/sites/cmm/migrated/documents/tranmer.pdf)

---
class: middle

## Data structures (7)

To sum up:

--

Hierarchical structures are generated by:

--

#### a) Data collection mechanism

--

- Survey data rarely comes from a simple random sample (SRS).

--

- Surveys often have multi-stage designs: clustered data.

--

#### b) ‘Natural’ structures within the population

--

- Individuals are clustered according to geography, household, etc.

--

- Observation are therefore not independent.

---
## Why is non-independence a problem?

--

If data has been collected using a two-stage design, carrying out an  individual level analysis is equivalent to assuming it is a simple random sample.

But independence assumption is unrealistic; for example:

- we could  expect a positive correlation between exam scores from pupils from the same school.

--

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/structure5.PNG" width="40%" style="display: block; margin: auto;" /&gt;

--

- The very fact that two pupils go to the same schools makes them have a "shared environment"

---
## What is a level? (1)

--

A study samples 100 areas from the UK. Within each area a  sample of citizens is selected and asked questions. Below are  shown the first six rows of the resulting dataset.

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/whatislevel.PNG" alt="Source: LEMMA course Bristol University" width="40%" /&gt;
&lt;p class="caption"&gt;Source: LEMMA course Bristol University&lt;/p&gt;
&lt;/div&gt;
--

Question time:
  
- Which of the variables can be sensibly treated as random  classifications and taken as levels of units?
  
---
## What is a level? (2)

--

We are interested in assessing to what extent the difference  between boys’ and girls’ educational achievement varies across  secondary schools for 16 year old students.

--

Which of the following designs would be most effective?

--

1. Randomly sample 5 schools and take achievement scores for  100 boys and 100 girls aged 16 in each school.

--

2. Randomly sample 30 schools and within each school take a  random sample of 10 boys and 10 girls aged 16 and take these  children’s achievement scores.

--

3. Sample 1000 schools and take 1 boy and 1 girl aged 16 from  each school.

---
## What is a level? (3)

--

Should a variable be treated as a level in a multilevel structure or as a categorical explanatory variable?

--

Consider a structure with students nested in schools with information on school type (state vs private). How should we include school type in our model?

--

1) As a **random classification** (i.e. level) if units can be regarded as a random sample from a wider population of units, for example schools. 

  - Interested in generalising to population of schools.
  
--

2) As a **fixed classification** (i.e. categorical variable) if small fixed number of categories. For example, if state and private schools were not two types sampled from a larger number of types. 

  - Not  interested in generalising to a wider population of school types.
  
--

Schools can be thought of as a **random classification** but school type should be a **fixed classification**


---
class: middle

--

## Why does clustered data matter?

--

Standard analysis assumes independence and estimates standard  errors of model parameters accordingly.

--

If observations within clusters are positively correlated this will underestimate standard errors.

--

- Result: variables may appear significant when in fact they are not.

--

- What to do then? Need to take account of clustering.

---

class: middle

--

## Why are standard errors too small if we ignore clustering?

--

Suppose we have 5000 individuals in 100 groups.

--

In a single level model standard errors calculated on the  assumption that the sampled individuals provide 5000 independent  pieces of information.

--

But when outcomes are clustered, the number of independent observations (the effective sample size - ESS) will be fewer than 5000  and standard errors from standard regression will be too small.

--

ESS depends on the amount of clustering. For example, if all individuals in a group have the same y value, ESS = 100.

--

Underestimation most severe for coefficients of level 2 variables.

---
class: middle

## What's the problem with standard regression (OLS)?

--

### Technical

--

Assumption of independence of residuals will be invalid if there are dependencies between individuals in the same group (area, school  etc). This will lead to underestimation of standard errors, and therefore p-values that are too small.

--

### Substantive

--

We are often interested in estimating the amount of variation between groups, and the extent to which it can be explained by  group-level explanatory variables.

---

class: middle

## Methods for hierarchical structures

--

Traditional approaches to analysing clustered data treat clustering as a nuisance that must be accounted for.

--

Parameters estimated in the usual way but standard error estimates are adjusted for impact of clustering.

--

Multilevel modelling takes account of hierarchical structure and regards structure of substantive interest.

---

class: middle

## General framework notation (1)

--

- Level one/microlevel unit: `\(i\)` (e.g., individual)

--

- Level two/macrolevel unit: `\(j\)` (e.g., area, school)

--

- There are `\(i=1,2, ... n_j\)` level one units within level-2 units 

--

- and `\(j=1,2, ... J\)` level-2 units.

--

- Response variable `\(y_{ij}\)` (continuous) is a function of:

  - individual variables: `\(x_{0ij}, x_{1ij}, ... x_{pij}\)`
  
  - and contextual variables: `\(z_{1j},z_{2j},…z_{Qj}\)`
  
--

- Error terms: 

    - `\(e_{ij}\)` at the individual level
    
    - `\(u_{0j}\)` at level-2

---
class: middle

## Multilevel modelling software

--

- R packages: **lme4**, nlme, R2MLwiN, rstanarm, brms, MCMCglmm (and others)

--

- Stata: mixed (previously known as xtmixed)

--

- SAS: PROC MIXED

--

- SPSS: MIXED and VARCOMP commands

--

- MLwiN (specialist software)

--

- Mplus, Latent Gold and others!

---

class: left, middle, inverse

### &lt;font color="white"&gt;Part Three: &lt;/font&gt;

# Variance components and group-specific estimates

---
### Single level regression for the mean (1)

--

Consider the simplest possible statistical model:

--

`$$\LARGE y_i = \beta_0 + e_i$$`
where `\(\beta_0\)` is the mean of `\(y\)` in the population, and `\(e_i\)` is the residual for the i-th individual `\((i = 1, 2,...n)\)`

--

- Usually assume `\(e_i\)` **~** `\(N(0,\sigma^2)\)`.

    - Normal distribution
    
--

- The variance `\(\sigma^2\)` summarizes the variability around the mean;

    - if this is zero all the points would lie on the `\(y=\beta_0\)` line

---
### Single level regression for the mean (2)

--

These are the residuals for four data points in single model for the mean:

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/resid1.PNG" width="60%" style="display: block; margin: auto;" /&gt;
---
## Multilevel model for group means (1)

--

`\(y_{ij}\)` is the value of `\(y\)` for the i-th individual in the j-th group. A model that allows for (random) group effects is:

--

`$$\LARGE y_{ij} = \beta_0 + u_{0j} + e_{ij}$$`
--

- `\(\beta_0\)` is the overall mean of `\(y\)` (across all groups).

--

- `\(\beta_0 + u_j\)` is the mean of `\(y\)` for group j.

--

- `\(u_j\)` is the difference between group j’s mean and the overall mean.

--

- `\(e_{ij}\)` is the difference between the y-value for the i-th individual and that individual’s group mean:

    - `\(e_{ij} = y_{ij} - (\beta_0 + u_j)\)`

---
## Multilevel model for group means (2)

Individual and group residuals in a two level model for the mean:

--

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/resid2.PNG" width="60%" style="display: block; margin: auto;" /&gt;

---
## Multilevel model for group means (3)

Example: Weight in small areas in London: 

--

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/resid3.PNG" width="70%" style="display: block; margin: auto;" /&gt;

---
## Multilevel model for group means (4)

Example: Weight in small areas in London: 

--

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/resid4.PNG" width="70%" style="display: block; margin: auto;" /&gt;

What's changed from the previous example?

---
## Variance Partitioning (1)

--

Assume `\(u_j\)` **~** `\(N(0,\sigma_u^2)\)` and `\(e_{ij}\)` **~** `\(N(0,\sigma_e^2)\)`

--

`\(\sigma_u^2\)` is the between group variance and `\(\sigma_e^2\)` is the within group variance.

--

The variance partition coefficient is the proportion of total variance due to differences between groups.

--

`$$VPC = \frac{\sigma_u^2}{\sigma_u^2 + \sigma_e^2}$$`
--

- VPC is also denoted as `\(\rho\)` (rho)

--

- Also known as the Intra-class correlation (ICC)

--

- `\(VPC = 0\)` if no group effects 

- `\(VPC = 1\)` if no within-group differences

---
## Variance Partitioning (2)

--

.pull-left[

Example "tutorial" data: subset of a large dataset of examination results in London schools


- Empty MLM results:

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/vpc2.PNG" width="100%" style="display: block; margin: auto;" /&gt;

]

--

.pull-right[

VPC:

`$$\rho = \frac{\sigma_u^2}{\sigma_u^2 + \sigma_e^2}$$`
`$$\rho = \frac{0.169}{0.169 + 0.848}$$`
`$$\rho = 0.166$$`
**16.6%** of the total variation in exam scores is due to differences between schools.
]

--

Assuming `R2MLwiN` and `lme4` are installed and loaded, this is to replicate the above example:

```{}
data(tutorial)
summary(mlm1 &lt;- lmer(normexam ~ 1 + (1|school), data=tutorial, REML=F))
```
---
## Variance Partitioning (3)

--

- Which model has the highest VPC?


&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/vpc1.PNG" width="70%" style="display: block; margin: auto;" /&gt;

--

- Answer: The figure on left-hand side has less within-group variation, hence VPC is larger

---
## Testing for group effects (1)

--

- Test `\(H_0: \sigma_u^2 = 0\)` by comparing single level and multilevel model in a likelihood ratio test.

--

- Use the deviance statistic:

`$$D = 2(logL_1 - logL_2)$$`
--

- Where:

    - `\(L_1\)` is the likelihood of the single level model
    - `\(L_2\)` is the likelihood of the multilevel model

--

- The test statistic `\(D\)` is compared with a `\(\chi^2\)` distribution with `\(df\)` = number of extra parameters in the more complex model

--

- Rejection of the null hypothesis implies “real group differences", in which case the multilevel model is preferred.

---
## Testing for group effects (2)

Example "tutorial" data

- First we run a single-level model and extract the loglikelihood:

```r
single &lt;- lm(normexam ~ 1, data=tutorial)
(L1 &lt;- logLik(single))
```

```
## 'log Lik.' -5754.682 (df=2)
```

- Then we run the multilevel model and extract the loglikelihood:

```r
mlm1 &lt;- lmer(normexam ~ 1 + (1|school), data=tutorial, REML=F)
(L2 &lt;- logLik(mlm1))
```

```
## 'log Lik.' -5505.324 (df=3)
```

--

.pull-left[

Apply the formula and compare to a `\(\chi^2\)` distribution with `\(df\)` being the number of extra parameters:


`$$D = 2*(L_2 - L_1)$$`
]

--

.pull-right[
**Quick tip:** for a difference of 1 parameter, `\(\chi^2\)` values over 3.84 are statistically significant
]
---

class: left, middle, inverse


# Practical 1


---

class: left, middle, inverse

### &lt;font color="white"&gt;Part Four: &lt;/font&gt;

# Accounting for individual and group characteristics: fixed effects

---
## Random Intercepts model (1)

--

- We will now add a level 1 explanatory variable to the model:

--

`$$\LARGE y_{ij} = \beta_0 + \beta_1 x_{ij} + u_j + e_{ij}$$`
--

- The overall relationship between `\(y\)` and `\(x\)` is represented by a straight line with intercept `\(\beta_0\)` and slope `\(\beta_1\)`

--

- There are two components in this model:

--

    -	Fixed part: `\(\beta_0 + \beta_1 x_{ij}\)`

    - Random part: `\(u_j + e_{ij}\)`

---
## Random intercepts model (2)


Same slope in all areas (same relationship between age and weight  in all areas)

--

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/randint1.PNG" width="70%" style="display: block; margin: auto;" /&gt;

---
class: middle

## Random intercepts model (3)


`$$\LARGE y_{ij} = \beta_0 + \beta_1 x_{ij} + u_j + e_{ij}$$`

--

- Suppose now that `\(x\)` is a dichotomous predictor: only takes values 0 and 1

--

- `\(\beta_0\)` is the overall mean of `\(y\)` for individuals with `\(x=0\)`

--

- `\(\beta_0 + u_j\)` is the mean for individuals with `\(x=0\)` in group j

--

- The slope `\(\beta_1\)` is the difference in the mean for `\(x=1\)` relative to `\(x=0\)` (in any group)

---
## Random intercepts model (4)

.pull-left[
Example "tutorial" data:

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/randint2.PNG" width="100%" style="display: block; margin: auto;" /&gt;
]

--

.pull-right[

- Coefficient for **standlrt** is 0.563

  - For each one-unit increase in standlrt we can expect a 0.563 standard deviations increase in **normexam**
  
- Is the coefficient statistically significant?

  - Where are the p-values?
  
  - **Tip:** t-values over 1.96 correspond to p-values below 0.05

]

To reproduce this example:
```{}
summary(mlm2 &lt;- lmer(normexam ~ standlrt + (1|school), data=tutorial, REML=F))
```
---
class: middle

### Effect of a level-1 variable on the variance

--

- Adding a level 1 explanatory variable to the model will always  reduce the level 1 variance and the total variance.

--

- However, the level 2 variance may stay the same, increase or  decrease.

--

- This depends on the association between the level 1 explanatory  variable and the level 2 outcomes.

---
### Adding level 2 explanatory variables (1)

--

MLM allows us to explore the effects of group-level variables while simultaneously allowing for **unmeasured** group characteristics that influence the outcome.

--

- But it also allows for **measured** group characteristics:

--

When modelling individuals at level 1 and groups at level 2: 

- level 2 variables are often called **contextual variables** 

- and their effects on an individual’s y-value are **contextual effects**.

--

**Note: it is particularly important to use MLM to estimate contextual effects because their standard errors may be severely underestimated when a single-level model is used.**

---
### Adding level 2 explanatory variables (2)

.pull-left[
Example "tutorial" data:

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/randint3.PNG" width="100%" style="display: block; margin: auto;" /&gt;
]

--

.pull-right[

- The reference category for **schgend** is mixed school

- What type of school obtained better scores?


]

--

To reproduce this example:
```{}
summary(mlm3 &lt;- lmer(normexam ~ standlrt + factor(schgend) + 
        (1|school), 
        data=tutorial, REML=F))
```
---
### Contextual effects: Example research questions

--

- How do teacher characteristics (for example, the number of years in teaching or measures of their teaching style) affect student attainment?

  - Is a student’s attainment affected by the ability of peers, and does any effect depend on a student’s own ability?

--

- Is living in a deprived area associated with poorer health?


  - Is this association independent of personal deprivation?

--

- What is the role of family background on child health?

---
class: middle

## Sources of contextual data

--

- Data referring to organisations may be collected routinely by government authorities, 

  - Administrative data
  
  - GIS data

--

- Contextual data may also derive from level 1 data that is aggregated to form level 2 variables.

  - e.g. School averages

--

- Data may be collected at level 2, for example surveys in which key members of a group are interviewed.

---
class: middle

### Multilevel model with contextual effects

- Include a level 2 variable in exactly the same way as a level 1 variable.

--

- Suppose `\(x_{ij}\)` is defined at level 1 and `\(x_{2j}\)` at level 2. The random intercept model is:


`$$\LARGE y_{ij} = \beta_0 + \beta_1 x_{ij} + \beta_2 x_{2j}+ u_j + e_{ij}$$`

--

**Note:** Sometimes z's may be used to denote level-2 explanatory variables, but you can't go wrong with x.

---

class: left, middle, inverse


# Practical 2

---
class: left, middle, inverse

### &lt;font color="white"&gt;Part Five: &lt;/font&gt;

# Multilevel modelling for binary responses
  
---

class: middle

## This session's goals

- Introduce the MLM for binary responses

--

- Provide examples of real-world research

--

- Examine statistical output

--

- Practice fitting a logistic MLM

---
## When to use Multilevel models for binary response? (1)

### Recidivism Study in Chile

.pull-left[


**Data**: criminal history data of a cohort of individuals released from prison 

**Outcome**: Whether a *person* got a new conviction or not

**Levels**: *Individuals* (level 1) nested within *prisons* (level 2)

]

.pull-right[


&lt;img src="https://raw.githubusercontent.com/A-mora/MLM_summer-school/main/images/hedi-benyounes-G_gOhJeCpMg-unsplash.jpg" width="90%" align="right"/&gt;

]

&lt;br&gt;

**Morales-Gomez, A.** (2018). [Individual and Structural Factors Affecting Recidivism: The Role of Prisoners, Prisons and Places in the Chilean Context](https://www.research.manchester.ac.uk/portal/files/77567623/FULL_TEXT.PDF)
---

## When to use Multilevel models for binary response? (2)

### Controlled Delivery of Drug Parcels in Scotland

.pull-left[



&lt;img src="https://raw.githubusercontent.com/A-mora/MLM_summer-school/main/images/markus-spiske-BPxkU4uPq6Y-unsplash.jpg" width="90%" align="left"/&gt;

]

.pull-right[


**Data**: Drug parcels seized by the UKBF en route to Scotland

**Outcome**: Whether a *parcel* was adopted for a controlled delivery or not

**Levels**: *drug parcels* (level 1) nested within *Local Authorities* (level 2)

]

&lt;br&gt;

**Morales-Gómez, A., McVie, S. &amp; Pantoja, F.** (2022). [Controlled Delivery of Illegal Drug Parcels in Scotland: Does Policing Practice Align With a Public Health Approach Focused on Drug-Related Harm?](https://journals.sagepub.com/doi/full/10.1177/00220426221098986)

---

#.center[Multilevel models for binary response]

The multilevel linear model is generally appropriate when the outcome is continuous and
normally distributed

--

Other types of data do not satisfy the assumption of normality: Count data, categorical data, ordinal data, etc. 

--

Just as in the well-known single level models, we use logistic regression in multilevel modeling when the outcome variable is binary (Yes/No)

---
class: middle

## Examples

--

- Whether someone reoffends or not

--

- Whether someone was victim of a crime or not

--

- Whether someone intends to vote in the next elections


---
##  Logistic regression (Recap)

Assume we want to analyse employment patterns in a sample of people:

--

We have binary variable `\(Y\)` indicating employment status where:
 - 1 = employed
 - 0 = unemployed

--

We can't use linear regression as many of the assumption for linear regression are not satisfied

--
- Values are bounded by 0 and 1 

--

- We can't assume normality 

--


We model the probability that `\(y = 1\)` (i.e. employed)

`$$Pr(y_i=1) = p_i$$`

---
class: middle

## Logistic regression (Recap)

--

We use a logistic or **logit** transformation of the outcome to *link* the dependent variable to a set of explanatory variables

`$$logit(p_i)=\log\left(\frac{p_{i}}{1-p_{i}}\right)$$`
--

We can write the model:
`$$logit(p_i)=\log\left(\frac{p_{i}}{1-p_{i}}\right) =  \beta_0+\beta_1x_{i}$$`
--

The logit link function keeps values in a range between 0 and 1

- This is necessary as probabilities must be between 0 and 1

--

- We can write the equation in terms of odds:

`$$\frac{p_{i}}{1-p_{i}} =  \exp(\beta_0+\beta_1x_{i})$$`
---
class:middle

## Two level random intercept model for binary response

--

Using the same example, imagine that we now have an additional variable indicating local authorities where respondents live

--

Consider a two-level structure where `\(n\)` individuals `\(i\)` are nested within groups `\(J\)`
- Group can be any level-2 unit (school, cities, etc.)

--

.pull-left[

`$$\log\left(\frac{p_{ij}}{1-p_{ij}}\right)  = \beta_0+\beta_1x_{ij}+u_j$$`
&lt;br&gt;
&lt;br&gt;
where
&lt;br&gt;
&lt;br&gt;
`$$u_j \sim N(0,\sigma^2_u)$$`

]

.pull-right[

The left side is the nonlinear transformation (log odds)

the right side takes the form of a linear model

&lt;br&gt;

The group effects or level-2 residuals `\(u_j\)` are assumed to be independent and follow a normal distribution

]
---
class: middle

## Interpretation of main parameters

--

`\(\beta_0\)` is the *overall intercept*

--

- `\(\beta_0\)` is the log-odds that `\(y=1\)` when `\(x=0\)` and `\(u=0\)`

--

`\(\beta_1\)` is usually referred to as the *cluster-specific* effect

--

- `\(\beta_1\)` is the effect of a 1-unit change in `\(x\)` in the log-odds that `\(y=1\)`, while adjusting for group effect `\(u\)`

--

`\(u_j\)` is the *group random effect* or level two residual

--

- `\(\beta_0 + u_j\)` is the intercept for a given group `\(j\)`

--

- `\(var(u_j) = \sigma_u^2\)` is the between-group variance and represents the variability across groups


---
class: middle
## Variance Partition Coefficient (VPC)

--
Measures the proportion of the total variance that is due to differences between groups

--

There are several ways of defining VPC for binary data:
- Model linearisation
- Simulation
- .bold[&lt;font color="#314152"&gt;Latent Variable approach&lt;/font&gt;]

--

For more details: [click here](https://rss.onlinelibrary.wiley.com/doi/epdf/10.1111/j.1467-985X.2004.00365.x?saml_referrer)

.center[&lt;img src="https://raw.githubusercontent.com/A-mora/MLM_summer-school/main/images/VPC%20article.png", width="50%"/&gt;]


---
class: middle
### VPC  Latent Variable Approach

--

Recall VPC formula for continuous response:
`$$VPC = \frac{\sigma_u^2}{\sigma_u^2 + \sigma_e^2}$$`
--

Assume that the underlying response variable is *continuous* but we can only observe a **binary response** indicating whether the underlying variable is greater or less than a given threshold.

--

This underlying continuous variable comes from a logistic distribution with a variance of `\(\frac{\pi^2}{3} \approx 3.29\)`

--

Replacing on the previuos formula:

`$$VPC = \frac{\sigma_u^2}{\sigma_u^2 + \sigma_e^2*}$$`  
where `\(\sigma_e^2*= \frac{\pi^2}{3}\)`


---
## Example

### Antenatal care in Bangladesh

--

We want to analyse whether a woman received antenatal care from a medically-trained provider at least once before her most recent live birth. 

--

We also want to explore whether antenatal care **varies across communities**

--

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Variables &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Description &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Level &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; comm &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Community identifier &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; antemed &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Received antenatal care at least once  (1 = yes, 0 = no) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; mage &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Mother's age at the child's birth (in years) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; urban &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Type of region of residence  (1 = urban, 0 = rural) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

--

There are level 1 variable (individual level)  and level 2 variables (area level).
- antemed: Outcome 
- comm: Level 2 identifier
---


class: middle

##R output

```r
fit &lt;- glmer(antemed ~    # Outcome variable
             (1 | comm),  # level two specification
*            family = binomial("logit"),  ##link function
             data = mydata)
```

###**Fixed part**


```
Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)  
*(Intercept)  0.14809    0.07178   2.063   0.0391 *
---
Signif. codes:  
0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

```

###**Interpretation**

The log-odds of receiving antenatal care in an ‘average’ community `\((u_j=0)\)` is estimated as `\(\beta_0=0.148\)`.


---
class: middle

## R Output (2)

--

###**Random part**
```
Random effects:
 Groups Name        Variance Std.Dev.
*(comm)   (Intercept) 1.464    1.21    
Number of obs: 5366, groups:  comm, 361

```

--

###**Interpretation**

The intercept for community `\(j\)` is `\(0.148+u_j\)`, where the variance of `\(u_j\)` is estimated as `\(\sigma^2_u=1.464\)`

---
## Adding explanatory variables (1)

--



--

###**We add maternal age (level 1 predictor)**


```r
*fit2 &lt;- glmer(antemed ~ magec +
                (1 | comm), 
              family = binomial("logit"), 
              data = mydata)
```


```
Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
*(Intercept)  0.144604   0.071781   2.015    0.044 *  
*magec       -0.032357   0.005235  -6.181 6.37e-10 ***
---
Signif. codes:  
0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```

---
## Adding explanatory variables (2)




```r
*fit3 &lt;- glmer(antemed ~ magec + urban +
                (1 | comm), 
              family = binomial("logit"), 
              data = mydata)
```

&lt;br&gt;

--

###**We add Urban (level 2 predictor)**

```
Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.346247   0.074256  -4.663 3.12e-06 ***
magec       -0.032481   0.005221  -6.221 4.94e-10 ***
*urban        1.494405   0.132870  11.247  &lt; 2e-16 ***

```
---
class: middle

### A quick note on estimation of logistic MLM

--

- The models presented here are all fitted with frequentist methods

  - But...

--

- Researchers have previously found that Bayesian estimation performs better at estimating random effects with binary responses

--

- For some arguably dense details, see [Browne and Draper, 2002](https://users.soe.ucsc.edu/~draper/Browne-Draper2.pdf)


---
class: middle, inverse

## Practical 3

---

class: left, middle, inverse

### &lt;font color="white"&gt;Part Six: &lt;/font&gt;

# Differential processes between groups: random effects
  
---
## Quick revision (1)

### Variance components

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/resid3.PNG" width="70%" style="display: block; margin: auto;" /&gt;

---
## Quick revision (2)

### Random intercepts

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/randint1.PNG" width="70%" style="display: block; margin: auto;" /&gt;

---
## Random slopes model (1)

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/randslop1.PNG" width="70%" style="display: block; margin: auto;" /&gt;

---
## Random slopes model (2)

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/randslop2.PNG" width="70%" style="display: block; margin: auto;" /&gt;

---
## Random slopes model (3)

--

- Allow both intercept and slope to vary randomly across groups:
`$$\LARGE y_{ij} = \beta_0 + \beta_1 x_{ij} + u_{0j} + u_{1j} x_{ij} +e_{ij}$$`

--

- Comparing with the random intercepts model, a new term `\(u_{1j}x_{ij}\)` has been added and we have two random effects: `\(u_{0j}\)` and `\(u_{1j}\)`

--

- `\(\beta_1\)` is the slope of the average regression

--

- `\(\beta_1 + u_{1j}\)` is the slope of the line for group j

- Assume `\(u_{0j}\)` and `\(u_{1j}\)` follow a bivariate normal distribution with variances `\(\sigma_{u0}^2\)` and `\(\sigma_{u1}^2\)`, and covariance `\(\sigma_{u01}\)`

--
`$$\left[\begin{array}{ccc}
u_{0j} \\ u_{1j}
\end{array}\right] 
MVN \thicksim \left(\begin{array}{ccc}
\sigma_{u0}^2 &amp; \sigma_{u01}\\
\sigma_{u01} &amp; \sigma_{u1}^2
\end{array}\right)$$`

--

**Note:** Every time we add one random slope to our model, we have 2 additional coefficients.

---
class: middle

### Interpretation of intercept-slope covariance

- Positive values of `\(\sigma_{u01}\)` imply that groups with high intercept residuals `\(u_{0j}\)` tend to have slope residuals `\(u_{1j}\)`

- When `\(\sigma_{u01}&gt;0\)` groups with high intercepts (high `\(\beta_0 + u_{0j}\)`) tend to have steeper than average slopes (high `\(\beta_1 + u_{1j}\)`). Groups with low intercepts have flatter than average slopes.
  
      - This will lead to a “fanning out” of the group prediction lines.

- When `\(\sigma_{u01}&lt;0\)`, there is a “fanning in” pattern of group lines.

---
### Interpretation of differential slopes

.pull-left[

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/randslop2.PNG" width="70%" style="display: block; margin: auto;" /&gt;

- Suppose this is relationship between attainment at 16 ($y$) and attainment on entry to the school ($x$).


- Across range of `\(x\)`, school 1 is more effective than school 2.


]

--

.pull-right[

- But difference between the two schools widens as x increases.

- So choice of school is particularly important among children with high prior attainment.


- Although school 2 is less effective than school 1, it’s flatter line means that school 2 has decreased differences in the outcomes of  children with different prior attainments.


- Prior attainment is more  predictive of subsequent performance in school 1.
]

---
## Random slopes: example

.pull-left[
Example "tutorial" data:

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/randslop3.PNG" width="100%" style="display: block; margin: auto;" /&gt;
]

--

.pull-right[

- What is pattern of predicted school lines?

- What is the correlation between slope and intercept?


]

--

To reproduce this example:
```{}
summary(mlm4 &lt;- lmer(normexam ~ standlrt + factor(schgend) + 
                (1 + standlrt|school), data=tutorial, REML=F))
```
---
## School predicted lines

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/randslop4.PNG" width="70%" style="display: block; margin: auto;" /&gt;


---
## Is the addition of a slope meaningful?

.pull-left[

Example "tutorial" data

- First we run a random intercepts model:
```{}
mlm3 &lt;- lmer(normexam ~ standlrt + factor(schgend) + 
        (1|school), 
        data=tutorial, REML=F)
```

- Then we run the multilevel model with the random slope:
```{}
mlm4 &lt;- lmer(normexam ~ standlrt + factor(schgend) + 
        (1 + standlrt|school), 
        data=tutorial, REML=F)
```
]

.pull-right[

- Then we compare using the `anova` function

```{}
anova(mlm3, mlm4)
```

&lt;img src="https://raw.githubusercontent.com/patroncos/Summer_School_June22/main/images/randslop5.PNG" width="100%" style="display: block; margin: auto;" /&gt;

- The `\(df\)` for this test is 2 and the `\(\chi^2\)` value is `\(43.643\)`

- p-value is much smaller than `\(0.001\)`

]

---
## Centring and Standardising

--

- An explanatory variable with a mean of zero is achieved by subtracting the sample mean of x from the raw values, 

 - `\(x_i - \bar{x}\)`
 
 - This type of centring is sometimes called "grand mean centring".

  - After centring, the intercept is interpreted as the predicted mean of `\(y\)` at `\(\bar{x}\)` 

--

- Another way of achieving a mean of zero is by *standardising*

  - `\(\frac{x_i - \bar{x}}{s_x}\)`
  
  - You can do the same with `\(y\)` and all continuous x's
  
  - In this case, coefficients will be *standardised*, which means that interpretation is in units of standard deviations

--

- Why is this important?

  - `\(\sigma_{u0}^2\)` is the variance at `\(x=0\)`, which may be outside the range of `\(x\)`

---

class: left, middle, inverse


# Practical 4


---

class: left, middle, inverse


# Wrapping up


---
### What hasn't been covered today?

- There is a *plethora* of applications and model specifications we haven't covered. Just to name a few:

--

  - Longitudinal models

--

  - Bayesian models (all we've practiced today has been from a frequentist approach)

--

  - Non-hierarchical models: cross-classified, multiple membership

--
  
  - Spatial models

--

  - Multilevel social networks
  
--

MLMs are everywhere. Actually, once you see multilevel structures in your data, you cannot unsee them...

--

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://media.giphy.com/media/wc7RJ0QIrIXu0/giphy.gif" alt=" -'I see MLMs!'" width="160px" /&gt;
&lt;p class="caption"&gt; -'I see MLMs!'&lt;/p&gt;
&lt;/div&gt;

---
class: middle

## So where to go next?

--

- LEMMA (University of Bristol): Free online course. [Click here](http://www.bristol.ac.uk/cmm/learning/online-course/)

  - This course has tutorials in R, Stata and MLwiN

--

- Tutorials in R by Rens van de Schoot. [Click here](https://www.rensvandeschoot.com/tutorials/lme4) and [here](https://www.rensvandeschoot.com/tutorials/generalised-linear-models-with-glm-and-lme4/)

--

- An online tutorial covering this from a Bayesian perspective using R. [Click here](https://mc-stan.org/users/documentation/case-studies/tutorial_rstanarm.html)

--

- Online resources by UCLA's Statistical Methods and Data Analytics: [Click here](https://stats.oarc.ucla.edu/other/examples/ma-hox/) and [here](https://stats.oarc.ucla.edu/other/examples/ma-snijders/)

  - These are MLM textbook examples using R, Stata, MLwiN, Mplus and others.

---
### Some applications in social sciences (1)

**Education:**

- **Prior, L., Goldstein, H. &amp; Leckie, G.** (2021). [School value-added models for multivariate academic and non-academic outcomes: exploring implications for performance monitoring and accountability](https://www.tandfonline.com/doi/full/10.1080/09243453.2021.1919719)


- **Troncoso, P.** (2019). [A two-fold indicator of school performance and the cost of ignoring it](https://www.sciencedirect.com/science/article/pii/S0883035518313120)

- **Troncoso, P., Pampaka, M., Olsen, W.** (2016). [Beyond traditional school value-added models: a multilevel analysis of complex school effects in Chile](https://www.tandfonline.com/doi/full/10.1080/09243453.2015.1084010)

- **O'Hanlon, F., Paterson, L. &amp; McLeod, W.** (2013). [The attainment of pupils in Gaelic-medium primary education in Scotland](https://www.tandfonline.com/doi/abs/10.1080/13670050.2012.711807)


---
### Some applications in social sciences (2)

**Criminology:**

- **Morales-Gómez, A., McVie, S. &amp; Pantoja, F.** (2022). [Controlled Delivery of Illegal Drug Parcels in Scotland: Does Policing Practice Align With a Public Health Approach Focused on Drug-Related Harm?](https://journals.sagepub.com/doi/full/10.1177/00220426221098986)

- **Ben Matthews, Ben Collier, Susan McVie, S.** (2021). [Understanding digital drug markets through the geography of postal drug deliveries in Scotland](https://journals.sagepub.com/doi/full/10.1177/1477370821997323)

- **Morales-Gomez, A.** (2018). [Individual and Structural Factors Affecting Recidivism: The Role of Prisoners, Prisons and Places in the Chilean Context](https://www.research.manchester.ac.uk/portal/files/77567623/FULL_TEXT.PDF). (Prison effects: pp. 106-157). (Area effects: pp. 158-201)

- **Pina-Sánchez, J., Linacre, R.** (2013). [Sentence Consistency in England and Wales: Evidence from the Crown Court Sentencing Survey](https://doi.org/10.1093/bjc/azt040)

#### For more applications:

You could visit the University of Bristol's [Gallery of Multilevel Papers](https://www.cmm.bris.ac.uk/gallery/?_ga=2.180610919.176993313.1655119817-1241002600.1654809106)

---
class: middle 

## General multilevel modelling books


- **Goldstein, H.** (2011). Multilevel statistical models (4th ed.). John Wiley and Sons

- **Hox, J., Moerbeek, M., van de Schoot, R.** (2017). Multilevel Analysis: Techniques and Applications (3rd Ed). Routledge
 
- **Snijders, T., Bosker, R.** (2012). Multilevel Analysis: An Introduction to Basic and Advanced Multilevel Modeling (2nd ed.). Sage

---

class: left, middle, inverse


# Thank you!

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"countIncrementalSlides": false,
"highlightStyle": "github",
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
